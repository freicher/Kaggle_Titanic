{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/bildungscampus_logo.png' width=\"40%\" align=\"left\" />\n",
    "<img src='images/hhn.png' width=\"25%\" align=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/bar.png' width=\"100%\" align=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methodensteckbrief - LASSO Regression\n",
    "Data Science - Grundlagen | Fakultät Wirtschaft und Verkehr | Wirtschaftsinformatik - Informationsmanagement und Data Science | WS 2020/21\n",
    "\n",
    "***\n",
    "Daniel Messner (193842) - damessner@stud.hs-heilbronn.de <br>\n",
    "Sebastian Kahlert (196089) - skahlert@stud.hs-heilbronn.de <br>\n",
    "Sebastian Straßburg (208383) - strassburg@stud.hs-heilbronn.de\n",
    "***\n",
    "\n",
    "In diesem Methodensteckbrief wird die Funktionsweise der LASSO Regression erklärt. Neben der Formula werden ebenfalls die Modellprämissen und -gütemaßen als auch die Stärken und Schwächen der Regressionsmethode erläutert. Die interaktive Demonstration mit einem Beispieldatensatz schließt den Methodensteckbrief.\n",
    "\n",
    "<img src=\"images/einordnung.png\" alt=\"Einordnung in Data Science - Grundlagen\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geschichte\n",
    "Der Operator für die geringste absolute Schrumpfung und Selektion (LASSO) wurde von Tibshirani (1996) für die Parameterschätzung und gleichzeitig für die Variablen Modellselektion in der Regressionsanalyse eingesetzt (Vgl. Muthukrishnan und Rohini 2016). Unabhängig davon, wurde die LASSO Regression bereits in der geophysikalischen Literatur von Santosa u. Symes (1986) angewandt, zur Popularität hat jedoch Tibshirani (1996) maßgeblich beigetragen. Heute kommt die LASSO Regression vor allem in der Statistik im Allgemeinen und beim maschinellen Lernen zum Einsatz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernidee und Funktionsweise\n",
    "Lineare Regressionsmodelle dienen zur Vorhersage einer Zielvariablen. Dabei wird die Kausalität zwischen der Ziel- und zahlreicher Einflussvariablen überprüft. Die Einflussvariablen können einen schwachen, starken oder gar keinen Einfluss auf die Zielvariable besitzen. Mithilfe von zahlreichen Regressionsmethoden können die Kausalitäten untersucht werden. Bei der LASSO-Regression handelt es sich viel mehr um ein Verfahren, die die Anzahl der Einflussvariablen eines bestehenden linearen Regressionsmodells reduziert. Die Reduzierung der Einflussvariablen sowie die Funktionsweise und mathematische Herleitung wird im nachfolgenden Kapitel erklärt.\n",
    "\n",
    "> **Key Info** <br> Jedes lineare Regressionsmodell hat folgenden Aufbau: $\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1\\times x_1 + \\hat{\\beta}_2\\times x_2 + \\dots + \\hat{\\beta}_n \\times x_n + \\epsilon$. Dabei ist $\\hat{y}$ die Zielvariable, $\\hat{\\beta}_0$ der konstante Parameter (<i>Intercept</i>), $\\hat{\\beta}_n$ der Steigungsparameter (<i>Slope</i>), $x_n$ die Einflussvariable sowie $\\epsilon$ der Residualfehler (<i>Residuals</i>) bzw. die nicht erklärte Modellvarianz. Kennzeichen eines linearen Regressionsmodells ist, dass alle Einflussvariablen im 1. Grad betrachtet werden.  \n",
    "\n",
    "\n",
    "### Hintergrund\n",
    "Die Problematik aller Modelle, die im Bereich des maschinellen Lernens generiert werden, ist die Wahl der optimalen Modellkomplexität. In der Abbildung (in Anlehnung nach Deng u. a. 2015, S. 35) wird die Problematik im *Bias-Variance-Tradeoff* ersichtlich.\n",
    "* Beim **Bias** handelt es sich um die statistische Verzerrung, also um die Abweichung der Modellwerte von den Realwerten.\n",
    "* Bei der **Variance** handelt es sich um die Streuung der Daten um den Mittelwert. Je stärker die Daten um den Mittelwert streuen, desto höher ist die Varianz. \n",
    "* der **Total Error** setzt sich aus der Summe des **Bias** und der **Variance** zusammen. (Vgl. Singh 2018)\n",
    "\n",
    "![Bias-Variance-Tradeoff](images/Bias-Variance-Tradeoff.png)\n",
    "\n",
    "Je höher die Modellkomplexität (bzw. je höher die Anzahl der Einflussvariablen), desto niedriger ist der Bias und höher die Variance. Der Total Error nimmt ebenfalls zu. Ein Modell mit einer hohen Komplexität (*Overfitting*) erklärt zwar sehr gut die Trainingsdaten, jedoch kommt es zu häufigen Fehlerraten hinsichtlich der Testdaten. (Vgl. Singh 2018) Außerdem ist die Interpretierbarkeit solcher Modelle erheblich schwierig und schlecht anwendbar für weitere Berechnungen. Bei einem Modell mit niedriger Komplexität (*Underfitting*) erklärt es jedoch unzureichend die Grundgesamtheit. Das Modell sagt die Werte ungenau vorher. In der nachfolgenden Animation (in Anlehnung nach Rashidi u. a. 2019, S. 11) ist die Problematik dargestellt.\n",
    "\n",
    "> **Examplarisches Beispiel**:\n",
    "> Die Elektroautos von dem Unternehmen Tesla sind dafür bekannt, mithilfe von KI-Algorithmen autonomes Fahren zu ermöglichen. Eine Autonomie erfordert neben den Kameras, die rund um das Auto verbaut sind, auch entsprechende Algorithmen wie Mustererkennung und neuronales Denken. Mithilfe dieser Algorithmen kann ein Tesla Fahrzeug zum Beispiel die Verkehrsschilder klassifizieren. \n",
    "> Angenommen wir haben ein Modell (unter der Annahme, dass Tesla LASSO Regressionen verwendet), welches sich im optimalen Bereich befindet. Das Fahrzeug kann mit diesem Modell effizient autonom agieren. Es erkennt Verkehrsschilder und andere Umwelteinflüsse (Menschen, Wildtiere etc.). Würde das Modell nun erkennen wollen, welchen sozialen und kultruellen Hintergrund die Menschen haben, so wäre das Modell überangepasst (*Overfitting*), da es für das Auto irrelvant ist, ob jemand angloamerikanisch oder südasiatisch geprägt ist. Würde das Modell nur Verkehrsschilder erkennen, so würde er alle Menschen und Umweltfaktoren in seiner Umgebung nicht berücksichtigen, was fatale Folgen hätte. Insofern wäre das Modell nicht genug angepasst (*Underfitting*).\n",
    "\n",
    "![Underfitting vs. Overfitting](images/Underfitting_Overfitting.gif)\n",
    "\n",
    "Um die Modellkomplexität zu verringern, also die Anzahl der Einflussvariablen, gibt es 3 Möglichkeiten: \n",
    "* *Dimensionsreduktion* (*Verschmelzung* von ähnlich strukturierten Variablen)\n",
    "* *Variablenselektion* (Selektion einzelner Variablen unter Berücksichtigung bestimmter Kriterien)\n",
    "* *Regression Shrinkage* (Skalierung der Regressionsparameter). \n",
    "\n",
    "\n",
    "Die LASSO-Regression ermöglicht eine Verringerung der Modellkomplexität mittels *Regression Shrinkage*.\n",
    "\n",
    "> Das Ziel der LASSO-Regression im allgemeinen Kontext und für die nachfolgende Demonstration im Methodensteckbrief ist die Optimierung der Modellkomplexität (Reduzierung der **Variance**, Erhöhung des **Bias**), um die Fehlerrate der Testdaten zu reduzieren.\n",
    "\n",
    "![Optimization](images/Optimization.gif)\n",
    "\n",
    "### Formula\n",
    "Die LASSO Regression versucht, wie auch bei jeder Regression, die Summe der quadratischen Abweichungen zu minimieren. Durch diese Minimierung entsteht die bestmögliche Regression zwischen der Einfluss- und Zielvariablen. Im Zusammenhang mit dem maschinellen Lernen wird jedoch ein Schrumpfungsterm (*Shrinkage Penalty*) benötigt: $\\lambda \\sum_{j=1}^p |\\beta_j|$. Der Term bewirkt eine Skalierung aller Parameter des Regressionsmodells mit Ausnahme des konstanten Parameters (*Intercept*). (Vgl. James u. a. 2013, S. 219)\n",
    "\n",
    "Im Nachfolgenden ist die Forumla dargestellt (In Anlehnung nach Tibshirani 1996, S. 268; modifiziert nach James u. a. 2013, S. 219 - 220), die die LASSO Regression versucht zu minimieren:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\beta_0, \\beta_j}{\\arg\\min} \\Big\\{ \\sum_{i=1}^n\\Big(y_i - \\beta_0 \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 \\Big\\} \\textrm{    subject to } \\lambda \\sum_{j=1}^p |\\beta_j| \\leq s\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hierbei ist $n$ die Gesamtanzahl der Variablen, $p$ Gesamtanzahl der Werte einer Variable, $\\beta_0$ der konstante Parameter, $\\beta_j$ die Steigungsparameter der jeweiligen Variablen, $y_i$ die Zielvariable, $x_i$ die Einflussvariablen und $\\lambda$ der Sensibilitätsparameter für den Schrumpfungsterm. (Vgl. James u. a. 2013, S. 219) $arg min$ bedeutet, dass an der Stelle des Minimums (im Sinne der kleinsten Summe der quadratischen Abweichung) die Funktion berechnet werden soll. Hierbei muss auch der Schrumpfungsterm gleichermaßen mit berücksichtigt und erfüllt werden (*subject to*).\n",
    "\n",
    "Zur vereinfachten Darstellung wird folgende Formula (James u. a. 2013, S. 219) betrachtet (hierbei versucht die LASSO Regression die Formula zu minimieren): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{i=1}^n\\Big(y_i - \\beta_0 \\sum_{j=1}^p \\beta_jx_{ij}\\Big)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Die Skalierung mithilfe des Schrumpfungsterm bewirkt, dass Parameter mit minimalen Steigungen absolut auf 0, während Parameter mit größeren Steigungen nahezu auf 0 skaliert werden. Duch diese Schrumpfung der Parameter werden einzelne Variablen ausgeschlossen. Somit führt die LASSO Regression auch eine *Variablenselektion* durch. Wie sensitiv die Skalierung durchgeführt wird, hängt von der Größe des $\\lambda$ ab. Es gilt: je höher $\\lambda$, desto stärker ist die Skalierung der Steigungsparameter des Regressionsmodells. (Vgl. Tibshirani 1996, S. 270)\n",
    "\n",
    "> Mithilfe einer Cross-Validation kann der beste $\\lambda$-Wert identifiziert werden. Hierbei ist der beste $\\lambda$-Wert der Wert, wo für das Modell die höchste Modellgüte und das beste Informationskriterium vorliegt. Im nachfolgenden Kapitel werden die Modellgüte und Kriterien erläutert.\n",
    "\n",
    "\n",
    "### Modellgüte und Modellauswahl\n",
    "Für die Bestimmung der Güte eines Modells sowie der Modellauswahl werden verschiedene Maßen herangezogen. Nachfolgend werden 3 Maßen vorgestellt, die in der Demonstration verwendet werden:\n",
    "1. Das Bestimmtheitsmaß (Modellgüte) $R^2$ erklärt die Modellvarianz an der Gesamtvarianz. Je höher das Bestimmtsheitsmaß, desto besser erklärt das Modell die Regression multivariater Dimensionen. Je höher die Streuung der Werte, desto ungenauer wird die Regression erklärt und folglich sinkt $R^2$. Die Formula für $R^2$ ist (in Anlehnung nach: Backhaus u. a. 2016, S. 84):$$ \\begin{aligned}R^2=\\frac{SSE}{SST}=\\frac{\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\\end{aligned}$$ Hierbei ist $SSE$ die Modellvarianz und $SST$ die Gesamtvarianz. Für $SSE$ werden alle Variationen der Zielvariablen vom Regressionsmodell berechnet (daher $\\hat{y}$ als Schätzer); für $SST$ alle Variationen der Zielvariablen von den realen Werten (daher $y$ als Beobachtung).  \n",
    "\n",
    "\n",
    "2. Das Aikaike-Informationskriterium $AIC$ ist als Maß dafür geeignet, verschiedene Modelle miteinander zu vergleichen. Dabei besitzt das $AIC$ einen Strafterm - je höher die Anzahl der Parameter, desto sensitiver agiert der Strafterm. Das $AIC$ hat folgende Formula (in Anlehnung nach: Backhaus u. a. 2016, S. 333): $$\\begin{aligned} AIC=2\\times k - 2\\ln{\\mathcal{L}} \\end{aligned}$$ Dabei ist $k$ die Anzahl der Parameter und $\\ln{\\mathcal{L}}$ die logarithmierte Likelihood-Funktion. Dabei werden die wahrscheinlichsten Werte aus der Stichprobe für die Schätzung des Parameters betrachtet.\n",
    "\n",
    "\n",
    "3. Das Bayessche Informationskriterium ($BIC$) baut auf das $AIC$ auf. Der Unterschied ist der zugrundeliegende Strafterm. Statt der doppelten Multiplikation mit der Anzahl der Parameter erfolgt eine Multiplikation zwischen den logarithmierten Umfang der Stichprobe und der Anzahl der Parameter. Die Formula ist (in Anlehnung nach: Backhaus u. a. 2016, S. 333): $$ \\begin{aligned} BIC=\\ln{U}\\times k -2\\ln{\\mathcal{L}} \\end{aligned}$$ Hierbei ist $\\ln{U}$ der logarithmierte Stichprobenumfang.\n",
    "\n",
    "> Das $BIC$ bestraft den Term insgesamt stärker als das $AIC$ bei einer hohen Anzahl der Parameter. (Vgl. Backhaus u. a. 2016, S. 334) Es gibt jedoch kein optimales Informationskriterium, welches das beste Modell selektiert. Stattdessen werden beide Informationskriterien für die Modellauswahl angewandt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anwendungsvoraussetzungen\n",
    "Für die LASSO Regression gelten die gleichen Modellprämissen wie für jede multivariate lineare Regression: (Vgl. Backhaus u. a. 2016, S. 98, 111)\n",
    "1. **Linearität und korrekte Spezifizierung**: alle Variablen dürfen nur als 1. Grad vorliegen. Es darf keine Potenzierung der Werte stattfinden. Weiterhin muss ersichtlich sein, welche Variable(n) die Ziel- und Einflussvariable(n) darstellt. So gilt: $\\beta_0$ und $\\beta_n$ sind linear sowie für $X$ sind alle relevanten Einflussvariablen enthalten.\n",
    "\n",
    "\n",
    "2. **Homoskedastizität**: Die Variation der Residuen ist gleichmäßig ausgeprägt. Bei einer Heteroskedastizität folgt es zu einer ungleichmäßigen Variation der Residuen um das Regressionsmodell und folglich zu einer Verzerrung des Modells (*Underfitting*). Ein Rückschluss auf die Grundgesamtheit ist somit nicht mehr geeignet.\n",
    "\n",
    "\n",
    "3. **keine Multikollinearität**: Die Korrelationen innerhalb der Einflussvariablen beträgt: $Cor(X)<1$. Bei einer perfekten Korrelation von $Cor(X)=|1|$ würde eine vollkommende lineare Abhängigkeit vorliegen. Somit würde eine Redundanz innerhalb des Modells vorliegen und führt folglich zu einer Überanpassung.\n",
    "\n",
    "\n",
    "4. **Normalverteilung der Residuen**: Die Residuen müssen normalverteilt sein. Die Verteilung der Residuen um den Wert 0 muss gleichmäßig ausgeprägt sein. Liegt keine Normalverteilung vor, folgt es wie bei der 3. Annahme zu einer Verzerrung des Modells. Für die Überprüfung der Normalverteilung wird neben der grafischen Darstellung auch der Anderson-Darling-Test (Vgl. Anderson u. Darling 1952, S. 193) verwendet.\n",
    "\n",
    "\n",
    "5. **keine Autokorrelation**: Die Korrelation der Residuen beträgt: $Cor(\\mathcal{E})=0$. Bei einer Autokorrelation würden die Residuen bei einer Veränderung der Einflussvariable(n) linear gleichermaßen miterklärt werden. Dies führt zu einer Überanpassung (*Overfitting*) des Modells. Für die Überprüfung einer Autokorrelation wird der Durbin-Watson-Test (Vgl. Gujarati u. Porter 2009, S. 320 - 324) verwendet.\n",
    "\n",
    "> **Key Info** <br> In der Praxis ist es geläufig, dass nicht immer alle Anwendungsvoraussetzungen erfüllt sind. So kann ein gutes lineares Modell vorliegen mit heteroskedastischen Residuen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stärken und Schwächen\n",
    "Nachfolgend werden die Stärken und Schwächen der LASSO Regression tabellarisch zusammengefasst: (Vgl. Pereira u. a. 2016; Vgl. Fonti u. Belitser 2017)\n",
    "\n",
    "| Merkmal | Stärken | Schwächen |\n",
    "| :--- | :--- | :--- |\n",
    "| Variablenselektion | <ul><li>Schrumpfung von <i>nutzlosen</i> Variablen auf 0</li><li>Ausschluss aus dem Modell</li></ul> | <ul><li>willkürliche Auswahl von Merkmalen bei Gruppe von korrelierenden Merkmalen</li><li>möglicher Informationsverlust und geringere Genauigkeit des Modells</li></ul> |\n",
    "| Einfachheit | <ul><li>Einfache Interpretation des Modells durch Variablenselektion</li></ul> | <ul><li>starke Automatisierung des Modells vernachlässigt Modellanpassungen</li></ul> |\n",
    "| Informationsgehalt | <ul><li>Verringerung des Varianz</li><li>Anpassung der Modellkomplexität zwischen Verzerrung und Varianz</li></ul> | <ul><li>wichtige bzw. relevante Variablen können auch ausgeschlossen werden</li></ul> |\n",
    "| Rechenzeit | <ul><li>niedrigdimensionelle Datensätze erfordern weniger Rechenzeiten</li></ul> | <ul><li>hochdimensionelle Datensätze erfordern mehr Rechenzeiten</li></ul> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "In diesem Kapitel wird die Demonstration in Python und dazu die Interpretation der Ergebnisse sowie die Analogie zu der Formula bzw. zu den vorher erklärten Ergebnissen durchgeführt. Die Demonstration beinhaltet folgende Vorgehensweise:\n",
    "1. Packages\n",
    "2. Dataset und Splitting\n",
    "3. Datenbereinigung und Datenlage\n",
    "   1. Boxplot\n",
    "   2. Histogramm\n",
    "   3. Scatterplot\n",
    "   4. Waffle Plot\n",
    "4. Deklarierung der Einfluss- und Zielvariable\n",
    "5. Initalisierung des LASSO Modells und Cross-Validation\n",
    "   1. Bester $\\lambda$-Wert\n",
    "   2. Beste mittlere quadratische Abweichung ($MSE$)\n",
    "   3. Anzahl der Cross-Validation Splits\n",
    "6. Visualisierung der Cross-Validation\n",
    "   1. für $R^2$\n",
    "   2. für $AIC$ und $BIC$\n",
    "7. Modellerstellung\n",
    "   1. $R^2$ für Trainingsdaten\n",
    "   2. $R^2$ für Testdaten\n",
    "   3. Modell mit Intercept und Koeffizienten\n",
    "   4. Sortiertes Modell mit Koeffizienten\n",
    "8. Anwendungsvoraussetzungen\n",
    "   1. Linearität und korrekte Spezifizierug\n",
    "   2. Homoskedastizität\n",
    "   3. Multikollinearität\n",
    "   4. Normalverteilung der Residuen\n",
    "   5. Autokorrelation \n",
    "   6. usammenfassung der Anwendungsvoraussetzungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "Neben den ``numpy``-, ``matplotlib``-, ``pandas``- und ``seaborn``-Packages wird überwiegend auf die *Scikit-Learn* Umgebung (Pedregosa u. a. 2011) zurückgegriffen. Das Package ``warnings`` unterdrückt die Warnmeldungen innerhalb der Prozedur. Für alle Plots wird ein spezifisches Design von ``matplotlib.style`` verwendet.\n",
    "\n",
    "> Bei der standardmäßigen Installation sind alle Packages bereits enthalten. Das Package ``pywaffle`` ist die Ausnahme und muss nachträglich installiert werden. Die Installationsschritte sind unter folgendem Link zu finden: [pywaffle - PyPI](https://pypi.org/project/pywaffle/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausblenden von \"stoerenden\" Warnmeldungen\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.stattools as stm\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as sps\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn import linear_model, model_selection\n",
    "from sklearn.decomposition._nmf import EPSILON\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from pywaffle import Waffle \n",
    "\n",
    "# einheitliche Plot-Groesse\n",
    "plt.rcParams['figure.figsize'] = [15, 12]\n",
    "plt.subplots_adjust(left=0.1, bottom=0.7, right=0.6, top=1.5)\n",
    "\n",
    "# einheitlicher Plot-Stil\n",
    "import matplotlib.style as style\n",
    "style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset und Splitting\n",
    "Für die Demonstration wird der Rotweindatensatz betrachtet. Dieser wurde von Cortez u.a. 2009, heruntergeladen von Dua u. Gra 2017, erhoben. \n",
    "\n",
    "> Dabei stammt der Rotwein aus der nördlichen Region von Portugal (*Vinho Verde*). Neben dem Rotwein wurden auch Daten für Weißweine erhoben. Beide Weinsorten zusammen bilden 15% der Weinproduktion in Portugal sowie 10% des Weinexportes.\n",
    "\n",
    "Der Datensatz besteht aus 12 Attributen, wovon 11 Attribute quantitativ (metrisch kontinuierlich) und 1 Attribut qualitativ (ordinal) ausgeprägt sind. Folgende Attribute sind enthalten: (Vgl. Cortez u. a. 2009, S. 548 - 549; Vgl. Rhein-Ahr-Wein 2020; UCI Machine Learning 2017)\n",
    "* ``fixed acidity`` - feste Säure in $\\frac{g}{dm^3}$. Feste Säure ist ein Grundbestandteil jeden Weines, der aus den Trauben ursprünglich herkommt. Feste Säuren sind zum Beispiel Weinsäuren.\n",
    "* ``volatile acidity`` - flüchtige Säure in $\\frac{g}{dm^3}$. Die Säure verflüchtet sich schnell, zum Beispiel die Essigsäure.\n",
    "* ``critic acidity`` - Zitronensäure in $\\frac{g}{dm^3}$. Die Säure wird verwendet, um den Wein *frischer* zu machen.\n",
    "* ``residual sugar`` - Restzucker in $\\frac{g}{dm^3}$. Je eher die Gärung eines Weins gestoppt wird, umso höher ist der Zuckeranteil im Wein, da der Zucker nicht vollständig zum Alkohol abgebaut werden konnte.\n",
    "* ``chlorides`` - Chloride in $\\frac{g}{dm^3}$. Die Chloride sind Salzbausteine, die im Wein enthalten sind.\n",
    "* ``free sulfur dioxide`` - ungebundener Schwefeldioxid in $\\frac{mg}{dm^3}$. Ungebunden bedeutet, dass er keine Verbindungen mit anderen chemischen Elementen eingeht.\n",
    "* ``total sulfur dioxide`` - ungebundener und gebundener Schwefeldioxid in $\\frac{mg}{dm^3}$. Der gebundene Schwefeldioxid exisitiert in Verbindung mit anderen chemischen Elementen.\n",
    "* ``density`` - Dichte in $\\frac{g}{cm^3}$. Da der Wein zum größten Teil aus Wasser besteht, liegt die Dichte sehr nah an 1.\n",
    "* ``pH`` - pH-Wert. Der pH-Wert gibt an, wie hoch der saure oder basische Charakter ist. \n",
    "* ``sulphates`` - Sulfate in $\\frac{g}{dm^3}$. Genau wie beim Schwefeldioxid schützen die Sulfate vor dem Zerfall des Weins.\n",
    "* ``alcohol`` - Alkoholgehalt in $vol.\\%$.\n",
    "* ``quality`` - Qualität des Weins in einer Skala von 0 bis 10. Dabei wurde jeder Wein von mindestens 3 Gutachtern überprüft. Es wurde anschließend der Median für alle 3 Bewertungen des jeweiligen Weins berechnet und im Datensatz aufgenommen.\n",
    "\n",
    "> **Key Info** <br> Die Weinsorten lieblich, halbtrocken und trocken stehen für den Grad der Gärung. Ein lieblicher Wein ist zum Beispiel nicht vollkommen gegärt und schmeckt dadurch *süßer*. Ein trockener Wein dagegen enthält kaum Zucker und schmeckt dadurch *schwerer*. Schwefel ist zwar ein unnatürlicher Bestandteil, jedoch in allen Weinsorten zu finden. Der Schwefel verhindert den bakteriellen und oxydativen Zerfall des Weins und dient somit als *Konservierungsstoff*. Der pH-Wert schwankt in einer Skale von 0 bis 14. Je niedriger der Wert, desto stärker ist der saure Charakter.\n",
    "\n",
    "\n",
    "Im nächsten Schritt werden die Daten eingelesen und die ersten Zeilen des Datensatzes angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data = pd.read_csv(\"data/winequality-red.csv\", sep=\";\", decimal=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbereinigung und Datenlage\n",
    "Im weiteren Verlauf wird die Datenlage beschrieben mithilfe geeigneter Visualisierungen. In dem Datensatz sind keine fehlenden Werte vorhanden. Des Weiteren besteht der Datensatz aus 11 Attribute, die quantitativ (kontinuierlich), und 1 Attribut, welches qualitativ (ordinal) geprägt ist. Nachfolgend werden die Datentype der einzelnen Attribute dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot\n",
    "Mithilfe von ``ipywidgets`` kann die Datenlage für jedes Attribut erklärt werden. Boxplots eignen sich hierfür besonders gut. \n",
    "> Für die Attribute ``resiudal sugar``, ``chlorides``, ``total sulfur dioxide``, ``density`` und ``sulphates`` sind besonders viele Outliers zu erkennen. Für die Zielvariable ``quality`` sind die Qualitätsstufen 3 und 8 Ausreißer. Bislang gibt es keinen Rotwein, der die Qualitätsstufe 10 besitzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: (Vgl. Holscher u. a. 2020)\n",
    "def f(Attribut):\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.boxplot(x=Attribut, data=data).set(title=\"Boxplot: \" + Attribut, xlabel=\"\")\n",
    "\n",
    "interact(f, Attribut=list(data.columns));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramm\n",
    "Neben Boxplots können die Verteilung der Daten nach ihrer Anzahl in Histogrammen dargestellt werden. Mittels ``ipywidgets`` werden die Histogramme für jedes Attribut dargestellt.\n",
    "\n",
    "> Interessant zu beobachten ist, dass kein Attribut den Verlauf einer Normalverteilung hat. Entweder sind sie linksschief, rechtsschief oder sind in der Mitte breit gestreut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: (Vgl. Holscher u. a. 2020)\n",
    "def g(Attribut):\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.distplot(data[Attribut])\n",
    "\n",
    "interact(g, Attribut=list(data.columns));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot\n",
    "Mithilfe von Scatterplots können Beziehungen zwischen 2 Attributen dargestellt werden. Nachfolgend können für alle Attribute die Beziehungen beobachtet werden.\n",
    "> Eine Korrelation ist in den meisten Attributen nicht zu sehen. Die Korrelationsmatrix im Abschnitt Demonstration zeigt die Korrelationen zwischen einzelnen Attributen auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: (Vgl. Holscher u. a. 2020)\n",
    "def g(Attribut_x, Attribut_y):\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.scatterplot(x=Attribut_x, y=Attribut_y, data=data, hue=\"quality\")\n",
    "\n",
    "interact(g, Attribut_x=list(data.columns), Attribut_y=list(data.columns));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waffle Plot\n",
    "Der Waffleplot zeigt ideal die Ausprägungsmenge einzelner Attributwerte eines Attributs. Hierbei werden die Ausprägungen für die Zielvariable ``quality`` gezeigt. \n",
    "\n",
    "> Die meisten Rotweine haben eine Qualitätsstufe von 5 und 6, nachfolgend mit 7. Nur wenige Weine haben eine Qualitätsstufe von 3, 4 und 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Vgl. Li (2020 - PyWaffle Documentation)\n",
    "waffle_data = pd.DataFrame(data[\"quality\"].value_counts())\n",
    "waffle_data[\"label\"] = waffle_data.index\n",
    "waffle_data.head()\n",
    "\n",
    "fig = plt.figure(FigureClass = Waffle, rows = 10, columns=30, \n",
    "                 values = list(waffle_data[\"quality\"]), \n",
    "                 labels=list(waffle_data[\"label\"]),\n",
    "                 legend={'loc': 'upper left', 'bbox_to_anchor': (1, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deklarierung der Einfluss- und Zielvariable\n",
    "Wie bereits erwähnt wird die Zielvariable ``quality`` sein. Die Frage hierbei lautet: *Welche Einflussvariablen begünstigen eine gute oder schlechte Qualität des Rotweins?* Im Code werden alle Einflussvariablen als ``X`` gekennzeichnet, die Zielvariable mit ``Y``. \n",
    "\n",
    "Der Datensatz wird zudem in Trainings- und Testdaten gesplittet. Hierbei haben die Testdaten einen Anteil von 25\\% des Datensatzes; die Trainingsdaten dementsprechend 75\\%. Mithilfe der Angabe ``random_state=1234`` sind die Daten und die nachfolgende Analyse für den Zustand mit der Nummer ``1234`` reproduzierbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einfluss- und Zielvariable\n",
    "X = data.drop([\"quality\"], axis=1)\n",
    "Y = data[\"quality\"]\n",
    "\n",
    "# Daten splitten in Trainings- und Testdaten\n",
    "X, x, Y, y = train_test_split(X, Y, test_size=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisierung des LASSO Modells und Cross-Validation\n",
    "\n",
    "Um das LASSO Modell aufstellen zu können, muss vorher der beste $\\lambda$-Wert ermittelt werden. Hierbei werden verschiedene $\\lambda$-Werte in einer *Dictionary* zusammengefasst, beginnend mit 0 (entspricht einer normalen Regressionsanalyse ohne Schrumpfungsterm) bis hin zu 100. Mithilfe von ``GridSearchCV()`` lässt sich nun eine Cross-Validation (Unterteilung des Datensatzes in Teildatensätze, die abwechselnd als Test- und Trainingsdatensatz verwendet werden) durchführen (mit 5 Splittings). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Vgl. Peixeiro (2019)\n",
    "lasso = linear_model.Lasso()\n",
    "\n",
    "# Lambda Werte\n",
    "lambda_values = {\"alpha\": [0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 2, 5, 10, 25, 50, 100]}\n",
    "\n",
    "# Ermittlung des besten Lambda\n",
    "lasso_result = model_selection.GridSearchCV(lasso, lambda_values, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "lasso_result.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bester $\\lambda$-Wert \n",
    "Der Ergebnis zeigt, dass der beste $\\lambda$-Wert bei 0.0001 liegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beste mittlere quadratische Abweichung ($MSE$) \n",
    "Zu dem $\\lambda$-Wert beträgt die niedrigste Summe der durchschnittlichen quadratischen Abweichungen -0,4416."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_result.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anzahl der Cross-Validation Splits \n",
    "Insgesamt wurden 5 Splits für die Cross-Validation verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_result.n_splits_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Cross-Validation\n",
    "\n",
    "Die Cross-Validation kann ebenfalls visualisiert werden. Hierzu wurde der Trainings- und Testdatensatz betrachtet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### für $R^2$\n",
    "Der Kontrollparameter ist hierbei das Bestimmtheitsmaß $R^2$. Der beste $\\lambda$-Wert zeigt sich dort, wo die höchste Modellgüte (das höchste Bestimmtheitsmaß) ist. Am Anfang liegen die Test- und Trainingsdatensätze dicht beieinander, später weichen sie stärker ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Vgl. Kirenz (2019)\n",
    "# moegliche L-Werte\n",
    "lambda_values = (0, 0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1)\n",
    "\n",
    "# Listen fuer Cross-Validation Ergebnisse\n",
    "lambdas = []\n",
    "r2_train = []\n",
    "r2_test = []\n",
    "\n",
    "# Cross-Validation fuer jeden L-Wert + Speicherung\n",
    "for i in lambda_values:\n",
    "    reg = linear_model.Lasso(alpha=i)\n",
    "    reg.fit(X, Y)\n",
    "    r2_score = cross_val_score(reg, X, Y, cv=5, scoring=\"r2\")\n",
    "    lambdas.append(reg.coef_)\n",
    "    r2_train.append(reg.score(X, Y))\n",
    "    r2_test.append(reg.score(x, y))\n",
    "\n",
    "# Erstellung des DataFrame    \n",
    "results = pd.DataFrame(list(zip(r2_train, r2_test)), columns=[\"R2_Train\", \"R2_Test\"])\n",
    "results[\"L\"] = lambdas\n",
    "\n",
    "# Erstellung des Plots\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(results[\"R2_Train\"], color=\"red\", alpha=0.5, label=\"Train Dataset\", linewidth=3)\n",
    "plt.plot(results[\"R2_Test\"], color=\"green\", alpha=0.5, label=\"Test Dataset\", linewidth=3)\n",
    "plt.xticks(range(0, 8), labels=lambda_values)\n",
    "plt.title(\"Cross-Validation - $\\lambda$ und $R^2$\")\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### für $BIC$ und $AIC$\n",
    "Ein weiterer Kontrollparameter sind die Informationskriterien *BIC* und *AIC*. Hierbei wird der $\\lambda$-Wert genommen, wo die beiden Informationskriterien den niedrigsten Wert haben. In der Abbildung sind 2 Abweichungen zu erkennen. Beide jedoch grenzen die Wahl des besten $\\lambda$-Wertes ein zwischen 0.0001 und 0.001 ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Pedregosa, F u. a. (2020b)\n",
    "# BIC\n",
    "model_bic = linear_model.LassoLarsIC(criterion='bic')\n",
    "model_bic.fit(X, Y)\n",
    "alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "# AIC\n",
    "model_aic = linear_model.LassoLarsIC(criterion='aic')\n",
    "model_aic.fit(X, Y)\n",
    "alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "# Cross-Validation\n",
    "def plot_ic_criterion(model, name, color):\n",
    "    criterion_ = model.criterion_\n",
    "    plt.semilogx(model.alphas_ + EPSILON, criterion_, '--', color=color,\n",
    "                 linewidth=3, label='%s Kriterium' % name)\n",
    "    plt.axvline(model.alpha_ + EPSILON, color=color, linewidth=3,\n",
    "                label='niedrigster %s' % name)\n",
    "    plt.xlabel(r'$\\lambda$')\n",
    "    plt.ylabel('Kriterium')\n",
    "\n",
    "# Erstellung des Plots\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title(\"Informationskriterium für Modellauswahl\")\n",
    "plot_ic_criterion(model_aic, 'AIC', 'b')\n",
    "plot_ic_criterion(model_bic, 'BIC', 'r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellerstellung\n",
    "Nachdem der beste $\\lambda$-Wert ermittelt wurde, kann nun die LASSO Regression durchgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Modell mit besten Lambda\n",
    "reg_best_lambda = linear_model.Lasso(alpha=0.0001)\n",
    "reg_train = reg_best_lambda.fit(X, Y)\n",
    "reg_train_score = reg_train.score(X, Y)\n",
    "reg_test_score = reg_best_lambda.score(x, y)\n",
    "\n",
    "# LASSO Modell mit Koeffizienten\n",
    "modell = pd.DataFrame(list(zip(reg_train.coef_, X.columns)), columns=[\"Koeffizient\", \"Einflussvariable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ für Trainingsdaten\n",
    "Das Bestimmtheitsmaß liegt bei $R^2=0.3599$. Es werden demnach 35.99% der Daten mit dem Modell erklärt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ für Testdaten\n",
    "Das Bestimmtheitsmaß liegt bei $R^2=0.3572$. Es werden demnach 35.72% der Daten mit dem Modell erklärt. Beachte hierbei, dass die Modellgüten für die Test- und Trainingsdaten sehr ähnlich sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell mit Intercept und Koeffizienten\n",
    "Nun wird das Modell erstellt mit den konstanten Parameter $\\beta_0$ (Intercept) und den Steigungsparametern $\\beta_n$ der Einflussvariablen $X$. Basierend auf den Ergebnissen ergibt sich folgendes LASSO Regressionsmodell:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\begin{split}\n",
    "\\hat{y}=4.725531404346774 - 0\\times x_1 \\\\ -1.149796 \\times x_2  -0.167023 \\times x_3 \\\\ -0.000660 \\times x_4  -1.688271 \\times x_5 \\\\ +0.005306 \\times x_6  -0.003672 \\times x_7 \\\\ -0 \\times x_8  -0.549538 \\times x_9 \\\\ +0.911532 \\times x_{10} +0.289937 \\times x_{11}\n",
    "\\end{split}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Die Einflussvariable ``density`` hat hierbei ein Koeffizient von 0, wodurch die Variable ausgeschlossen wird. Demnach ergibt sich das folgende LASSO Regressionsmodell:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "        \\begin{split}\n",
    "\\hat{y}=4.725531404346774  -1.149796 \\times x_2 \\\\  -0.167023 \\times x_3  -0.000660 \\times x_4 \\\\  -1.688271 \\times x_5  +0.005306 \\times x_6 \\\\ -0.003672 \\times x_7  -0.549538 \\times x_9 \\\\ +0.911532 \\times x_{10} +0.289937 \\times x_{11}\n",
    "\\end{split}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_best_lambda.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortiertes Modell mit Koeffizienten\n",
    "Um die relvenaten Einflussvariablen nach der LASSO Methode identifizieren zu können, empfiehlt es sich, die Koeffizienten nach absteigender Reihenfolge zu sortieren. Die Einflussvariablen ``sulphates``, ``chlorides`` und ``volatile acidity`` weichen besonders stark von der 0 ab. Dies kann mithilfe eines Boxplots ebenfalls dargestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modell.sort_values([\"Koeffizient\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots fuer Koeffizienten mit Ausreisser\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(modell[\"Koeffizient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben den Boxplot für die Koeffizienten können auch die Boxplots für die Ausreißer dargestellt werden. Hierzu werden im Code die Rechenvorschriften für die Berechnung der Ausreißer festgelegt. Anschließend wird in einer Funktion die Darstellung der Inputparameter mithilfe eines Widgets eingestellt.\n",
    "\n",
    "Die Formula für die Berechnung der Ausreißer lautet (hierbei ist $Q$ das Quantil, dementsprechend $Q_{25\\%}$ das 25%-Quantil): (Vgl. Hubert u. Vandervieren 2008, S. 5186)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{oberer Whiskers} = Q_{75\\%} + (1.5 \\times Q_{75\\%} - {Q_{25\\%}}) \\qquad \\textrm{unterer Whiskers} = Q_{25\\%} - (1.5 \\times Q_{75\\%} - {Q_{25\\%}})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots fuer Einflussvariablen als Ausreisser\n",
    "# @source: (Vgl. Holscher u. a. 2020)\n",
    "\n",
    "outliers = np.array(sorted(modell[\"Koeffizient\"]))\n",
    "down_Whisker = np.percentile(outliers,25) - (1.5 * (np.percentile(outliers,75) - np.percentile(outliers,25))) \n",
    "top_Whisker = np.percentile(outliers,75) + (1.5 * (np.percentile(outliers,75) - np.percentile(outliers,25))) \n",
    "outliers = list(outliers[(outliers < down_Whisker) | (outliers > top_Whisker)])\n",
    "modell_outliers = modell[modell[\"Koeffizient\"].isin(outliers)]\n",
    "\n",
    "def h(Outlier):\n",
    "    sns.set_context(\"talk\")\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.title(\"Boxplot: \" + Outlier + \" ~ \" + Y.name)\n",
    "    plt.xlabel(Y.name)\n",
    "    sns.boxplot(x=Y.name, y=Outlier, data=data, whis=np.inf, boxprops=dict(alpha=.3))\n",
    "    sns.stripplot(x=Y.name, y=Outlier, data=data, linewidth=1, alpha=1)\n",
    "\n",
    "interact(h, Outlier=list(modell_outliers[\"Einflussvariable\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vergleich der Regressionen\n",
    "Nachfolgend wird die LASSO Regression mit der linearen Regression verglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = sm.add_constant(X)\n",
    "model_ols = sm.OLS(Y, X2)\n",
    "model_ols_fit = model_ols.fit()\n",
    "model_ols_fit.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anwendungsvoraussetzungen\n",
    "Bei den Anwendungsvoraussetzungen werden die Modellprämissen des linearen Regressionsmodells überprüft. Da die LASSO Regression auf die lineare Regression aufbaut, wird im Folgenden eine lineare Regression durchgeführt, um zu überprüfen, ob die Daten und das Modell für die lineare und somit LASSO Regression geeignet sind.\n",
    "\n",
    "Am Anfang wird ein Datensatz erstellt, wo neben den tatsächlichen Werten (*actual values*) auch die vorhergesagten Werte (*predicted values*) und die Residualwerte (*residual values*) zusammengefasst werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Vgl. Macaluso (2018)\n",
    "# Initialisierung des linearen Regressionmodells\n",
    "assumptions = linear_model.LinearRegression()\n",
    "assumptions.fit(X, Y)\n",
    "\n",
    "# Erstellung der tatsaechlichen und vorhergesagten Werte sowie Residualwerte\n",
    "actual_values = Y\n",
    "predicted_values = assumptions.predict(X)\n",
    "values = pd.DataFrame({\"Tatsaechliche_Werte\": actual_values, \"Vorhergesagte_Werte\": predicted_values})\n",
    "values[\"Residual_Werte\"] = abs(values[\"Tatsaechliche_Werte\"]) - abs(values[\"Vorhergesagte_Werte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearität und korrekte Spezifizierung\n",
    "Die Linearität kann mithilfe der Visualisierung der tatsächlichen Werte mit den vorhergesagten Werten überprüft werden. Die tatsächlichen Werte sind die vorliegenden Werte der Zielvariablen. Die vorhergesagten Werte sind die Werte, die mithilfe des linearen Modells erzeugt werden. Weichen die tatsächlichen Werte mit den vorhergesagten Werten gleichmäßig von der linearen Regressionsgeraden ab, so liegt eine Linearität vor. (Vgl. Li 2018)\n",
    "\n",
    "> Im Plot ist zu erkennen, dass die vorhergesagten Werte relativ gleichmäßig von der Regressionsgeraden abweichen. Somit ist die 1. Voraussetzung erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"Tatsaechliche_Werte\", y=\"Vorhergesagte_Werte\", data=values, line_kws={\"color\" : \"green\"}, \n",
    "           height=8, aspect=15/10, x_jitter=.15, scatter_kws={'alpha':0.5})\n",
    "plt.title(\"Linearität\")\n",
    "plt.xlabel(\"Tatsächliche Werte\")\n",
    "plt.ylabel(\"Vorhergesagte Werte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homoskedastizität\n",
    "Die Homoskedastizität kann mithilfe einer Visualisierung dargestellt werden. Hierbei wird zu jedem Zeileneintrag des Datensatzes mithilfe des Indexes die Residualwerte um die horizontale Achse bei 0 dargestellt. Sind alle Residualwerte gleichmäßig um die Achse verteilt, so liegt eine Homoskedastizität vor.\n",
    "\n",
    "> Die Residuen weichen gleichmäßig über alle Beobachtungen hinweg um die horizontale Achse bei 0 ab. Somit ist die 2. Voraussetzung erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Homoskedastizität der Residuen\")\n",
    "plt.scatter(x=values.index, y=values[\"Residual_Werte\"])\n",
    "plt.axhline(0, xmin=0, xmax=1, linewidth=2, color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multikollinearität \n",
    "Mithilfe einer Korrelationsmatrix kann die Multikollinearität visualisiert werden. Dies erfolgt mit ``seaborn.heatmap``. Die Korrelationen liegen hierbei zwischen -1 und 1. Je näher die Korrelationen an der -1 und 1, desto stärker ist die lineare Abhängigkeit zweier Einflussvariablen zueinander.\n",
    "\n",
    "> In der Korrelationsmatrix wird ersichtlicht, dass eine relativ starke negative Korrelation zwischen ``pH`` und ``fixed acidity``, ``critic acid`` und ``volatile acidit``, ``pH`` und ``critic acid`` sowie ``alcohol`` und ``density`` vorliegt. Da die Korrelationen jedoch nicht $Cor(X)=1$ betragen, liegt keine perfekte Korrelation vor und dementsprechend keine absolute Redundanz von Einflussvariablen innerhalb des Modells. Somit ist die 3. Voraussetzung (trotz einzelner starker Korrelationen) erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source: Vgl. Waskom (2020)\n",
    "plt.figure(figsize=(15,7))\n",
    "multi_corr = X.corr()\n",
    "mask = np.triu(np.ones_like(multi_corr, dtype=bool))\n",
    "sns.set_style(style=\"white\")\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(10, 250, as_cmap=True)\n",
    "sns.heatmap(multi_corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "            annot=True, annot_kws={\"size\": 7})\n",
    "plt.title(\"Multikollinearität via Korrelationsmatrix\")\n",
    "plt.xlabel(\"Einflussvariablen\")\n",
    "plt.ylabel(\"Einflussvariablen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalverteilung der Residuen\n",
    "Mithilfe von ``seaborn.distplot`` kann ein Histogramm für die Verteilung der Residualwerte erzeugt werden. Sind die Residualwerte gleichermaßen um die 0 verteilt, so liegt eine Normalverteilung vor. Paralell zur Normalverteilung wird der Anderson-Darling-Test verwendet mithilfe von ``scipy.stats.anderson``. Der Test berechnet einen Wert zur Aussage, ob eine Normalverteilung vorliegt oder nicht. Ist der errechnete Wert kleiner als die Werte für die Signifkanzniveaus $\\alpha = \\{1\\%, 2.5\\%, 5\\%, 10\\%, 15\\%\\}$, so liegt eine Normalverteilung mit zunehmender Wahrscheinlichkeit vor. Anderfalls liegt keine Normalverteilung vor.\n",
    "\n",
    "> Im Histogramm ist die Verteilung der Residuen dargestellt. Hierbei fällt auf, dass eine exakte Normalverteilung nicht vorliegt. Die Kurve hat im positiven Zahlenspektrum einen größeren Verlauf als im negativen Bereich. Der Anderson-Darling-Test berechnete eine Teststatistik in Höhe von $2.20546$ sowie die kritischen Werte für die jeweiligen Signifkanzniveaus. Die Teststatistik ist größer als alle anderen kritischen Werte. Demnach liegt ein Signifkanzniveau vor, was größer als $15\\%$ ist und somit eine Normalverteilung aufgrund niedriger Wahrscheinlichkeit auszuschließen ist. Somit ist die 4. Voraussetzung nicht erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use('bmh')\n",
    "sns.distplot(values[\"Residual_Werte\"])\n",
    "plt.title(\"Normalverteilung der Residualwerte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sps.anderson(values[\"Residual_Werte\"], dist=\"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autokorrelation\n",
    "Die Autokorrelation ist ein Phänomen, welches bei Zeitreihendaten auftretet. Eine Autokorrelation kann auch bei Querschnittsdaten auftreten, sofern eine Pfadabhängigkeit zwischen einzelnen Variablen besteht. Mithilfe von ``statsmodels.stats.stattools.durbin_watson`` kann der Durbin-Watson-Test durchgeführt werden. Hierbei liegt der Wert zwischen 0 und 4.\n",
    "* Je näher der Wert an der 0, desto wahrscheinlicher ist es, dass eine **positive** Autokorrelation vorliegt.\n",
    "* Je näher der Wert an der 4, desto wahrscheinlicher ist es, dass eine **negative** Autokorrelation vorliegt.\n",
    "* Je näher der Wert an der 2, desto wahrscheinlicher ist es, dass **keine** Autokorrelation vorliegt. (Vgl. Perktold u. a. 2020)\n",
    "\n",
    "\n",
    "> Beim Durbin-Watson-Test wurde ein Wert in Höhe von $2.0777$ berechnet. Da der Wert sich nah an der 2 befindet, liegt mit hoher Wahrscheinlichkeit keine Autokorrelation vor. Somit ist die 5. Voraussetzung erfüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stm.durbin_watson(values[\"Residual_Werte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zusammenfassung der Anwendungsvoraussetzungen\n",
    "Nachfolgend werden tabellarisch die Ergebnisse der Anwendungsvoraussetzungen sowie deren Folgen zusammengefasst:\n",
    "\n",
    "| Anwendungsvoraussetzung | Ergebnis | Folgen |\n",
    "| :--- | :--- | :--- |\n",
    "| Linearität und korrekte Spezifizierung | Parameter sind linear | lineares Modell |\n",
    "| Homoskedastizität | Residuen variieren gleichmäßig | keine Verzerrung des Modells |\n",
    "| Multikollinearität | vereinzelte starke Korrelation | keine Redundanz im Modell |\n",
    "| Normalverteilung der Residuen | keine Normalverteilung | Verzerrung des Modells |\n",
    "| Autokorrelation | keine Autokorrelation | keine Verzerrung des Modells |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung\n",
    "Neben den Daten für Rotweine sind ebenfalls auch Daten für Weißweine vorhanden. Führe mit den oben genannten Schritten ebenfalls eine vollständige Analyse mithilfe der LASSO Regression durch. Du findest den Datensatz unter ``data/winequality-white.csv``. Beachte folgende Schritte:\n",
    "1. Analysiere die Datenlage und führe ggf. eine Datenbereinigung durch.\n",
    "2. Untersuche die Daten mithilfe einer einfachen linearen Regression. Untersuche dabei die Koeffizienten der Einflussvariablen sowie die p-Werte. (Hinweis: je niedriger die Werte, desto wahrscheinlicher ist die Beziehung der Einflussvariable auf die Zielvariable.)\n",
    "3. Führe nun eine LASSO Regression durch. Ermittle dabei den besten $\\lambda$-Wert mithilfe einer Cross-Validation.\n",
    "4. Sind Ausreißer enthalten? Falls ja, um welche Einflussvariablen handeln es sich?\n",
    "5. Inwiefern können die Ergebnisse der einfachen linearen Regression und LASSO Regression miteinander verglichen werden?\n",
    "\n",
    "<!-- Code für Lineare Regression\n",
    "You crazy Cheater!\n",
    "# @source: https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(Y, X2)\n",
    "est2 = est.fit()\n",
    "est2.summary() -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literatur\n",
    "### Monographien \n",
    "> *Backhaus, Klaus u. a.* (2016). Multivariate Analysemethoden. Berlin, Heidelberg: Springer Berlin Heidelberg. <br><p></p>\n",
    "> *Gujarati, Damodar N. und Dawn C. Porter* (2009). Essentials of Econometrics. 4. Aufl. McGraw-Hill Education, S. 554. <br><p></p>\n",
    "> *James, Gareth u. a.* (2013). An Introduction to Statistical Learning. Bd. 103. Springer New York. \n",
    "\n",
    "### Zeitschriftenaufsätze \n",
    "> *Anderson, T. W. und D. A. Darling* (1952). “Asymptotic Theory of Certain ”Goodness of FitCriteria Based on Stochastic Processes”. In: The Annals of Mathematical Statistics 23.2, S. 193–212. <br><p></p>\n",
    ">*Cortez, Paulo u. a.* (2009). “Modeling wine preferences by data mining from physicochemical properties”. In: Decision Support Systems 47.4, S. 547– 553. <br><p></p>\n",
    ">*Deng, Bai-Chuan u. a.* (2015). “A new strategy to prevent overfitting in partial least squares models based on model population analysis”. In: Analytica Chimica Acta 880, S. 32–41. <br><p></p>\n",
    ">*Hubert, M. und E. Vandervieren* (2008). \"An adjusted boxplot for skewed distributions\". In: Computational Statistics & Data Analysis 52.12, S. 5186-5201. <br><p></p>\n",
    ">*Muthukrishnan, R und R Rohini* (2016). \"LASSO: A feature selection technique in predictive modeling for machine learning\". In: 2016 IEEE International Conference on Advances in Computer Applications, S. 18-20. <br><p></p>\n",
    ">*Pedregosa, F u. a.* (2011). “Scikit-learn: Machine Learning in Python”. In: Journal of Machine Learning Research 12, S. 2825–2830. <br><p></p>\n",
    "> *Pereira, Jose Manuel, Mario Basto und Amelia Ferreira da Silva* (2016). \"The Logistic Lasso and Ridge Regression in Predicting Corporate Failure\". In: Procedia Economics and Finance 39, S. 634 - 641. <br><p></p>\n",
    ">*Rashidi, Hooman H u. a.* (2019). “Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods”. In: Academic Pathology 6, S. 1-17. <br><p></p>\n",
    ">*Santosa, Fadil und William W. Symes* (1986). \"Linear Inversion of Band-Limited Reflection Seismograms\". In: SIAM Journal on Scientific and Statistical Computing 7.4, S. 1307-1330. <br><p></p>\n",
    ">*Tibshirani, Robert* (1996). “Regression Shrinkage and Selection via the Lasso”. In: Journal of the Royal Statistical Society 58, S. 267–288. \n",
    "\n",
    "### Software \n",
    "> *Holscher, Eric u. a.* (2020). Using Interact. <br><p></p>\n",
    "> *Li, Guangyang* (2020). PyWaffle Documentation. <br><p></p>\n",
    ">*Matplotlib development team* (2017). pylab_examples example code: broken_axis.py. <br><p></p>\n",
    "> *Pedregosa, F u. a.* (2020a). Lasso. <br><p></p>\n",
    ">— (2020b). Lasso model selection: Cross-Validation / AIC / BIC. <br><p></p>\n",
    ">— (2020c). sklearn.linear model.Lasso. <br><p></p>\n",
    ">— (2020d). sklearn.linear model.LassoCV. <br><p></p>\n",
    ">— (2020e). sklearn.linear model.LassoLarsIC. <br><p></p>\n",
    ">— (2020f). sklearn.model selection.cross_val_score. <br><p></p>\n",
    ">— (2020g). sklearn.model selection.GridSearchCV. <br><p></p>\n",
    ">— (2020h). sklearn.model selection.train_test_split. <br><p></p>\n",
    "> *Perktold, Josef, Skipper Seabold und Jonathan Taylor* (2020). statsmodels.stats.stattools.durbin watson. <br><p></p>\n",
    ">*Waskom, Michael* (2020). Plotting a diagonal correlation matrix.\n",
    "\n",
    "### Sonstige Schriften \n",
    ">*Dua, Dheeru und Casey Gra.* (2017). UCI Machine Learning Repository. (URL: http://archive.ics.uci.edu/ml) <br><p></p>\n",
    "> *Fonti, Valeria und Eduard Belitser* (2017). Feature Selection using LASSO. <br><p></p>\n",
    ">*Hayden, Andy* (2014). Find all columns of dataframe in Pandas whose type is .oat, or a particular type? (URL: https://stackoverflow.com/questions/21720022/find-all-columns-of-dataframe-in-pandas-whose-type-is-float-or-a-particular-typ) (besucht am 23. 10. 2020). <br><p></p>\n",
    ">*Kirenz, Jan* (2019). Lasso Regression with Python. Hrsg. von Jan Kirenz. (URL: https://www.kirenz.com/post/2019-08-12-python-lasso-regression-auto) <br><p></p>\n",
    ">*Li, Lorraine* (2018). Introduction to Linear Regression in Python. (URL: https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0) (besucht am 25. 10. 2020). <br><p></p>\n",
    ">*Macaluso, Jeff* (2018). Testing Linear Regression Assumptions in Python. (URL: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/) (besucht am 29.10.2020). <br><p></p>\n",
    ">*Peixeiro, Marco* (2019). How to Perform Lasso and Ridge Regression in Python. (URL: https://towardsdatascience.com/how-to-perform-lasso-and-ridge-regression-in-python-3b3b75541ad8) (besucht am 23. 10. 2020). <br><p></p>\n",
    ">*Rhein-Ahr-Wein* (2020). WEIN INHALTSSTOFFE: WAS IST ALLES DRIN? (URL: https://rhein-ahr-wein.de/blogs/weinwissen/wein-inhaltsstoffe) (besucht am 28. 10. 2020).  <br><p></p>\n",
    ">*Singh, Seema* (2018). Understanding the Bias-Variance Tradeoff, Hrsg. von Towards Data Science. (URL: https://towardsdatascience.com/understanding­the-bias-variance-tradeoff-165e6942b229) <br><p></p>\n",
    "> *UCI Machine Learning* (2017). Red Wine Quality. (URL: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009) (besucht am 04.11.2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
